{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================\n",
        "#\n",
        "#  NOTEBOOK FOR FINE-TUNING GEMMA 3 1B LOCALLY\n",
        "#\n",
        "# ------------------------------------------------------------------------------------------\n",
        "#\n",
        "#  **DISCLAIMER**\n",
        "#\n",
        "#  This notebook is intended for educational purposes only.\n",
        "#\n",
        "#  - Date: July 2025\n",
        "#  - Not suitable for production environments.\n",
        "#  - Use at your own risk.\n",
        "#\n",
        "# ==========================================================================================\n",
        "#\n",
        "#  Overview:\n",
        "#\n",
        "#  This notebook provides a step-by-step guide to fine-tuning the Gemma 3 1B model.\n",
        "#  The process involves:\n",
        "#\n",
        "#      01. Serving the Gemma 3 1B base model and getting a baseline answer.\n",
        "#      02. Fine-tuning the model with a custom dataset.\n",
        "#      03. Saving the fine-tuned adapter and the full merged model to local directories.\n",
        "#      04. Serving the fine-tuned model from local storage and comparing its answers\n",
        "#          with the base model.\n",
        "#\n",
        "# ------------------------------------------------------------------------------------------\n",
        "#\n",
        "#  Requirements:\n",
        "#  - A local or cloud environment (like Colab) with a GPU (e.g., NVIDIA L4).\n",
        "#  - A HuggingFace account that has acknowledged the Gemma 3 1B terms and a read-permission API token.\n",
        "#\n",
        "# ==========================================================================================\n"
      ],
      "metadata": {
        "id": "bOINTr8t25zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI7Owu6R2zu9"
      },
      "outputs": [],
      "source": [
        "# --- 1. Installation ---\n",
        "# Install necessary Python packages\n",
        "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes vllm\n",
        "\n",
        "# Restart the session to apply the newly installed packages\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Setup ---\n",
        "# Log in to your HuggingFace Account to download the model\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "## use your token "
      ],
      "metadata": {
        "id": "cEXxYQ1v25wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# --- 3. Configuration ---\n",
        "# Define model, dataset, and local directory paths\n",
        "\n",
        "# Use the instruction-tuned model as the base\n",
        "BASE_MODEL_ID = \"google/gemma-3-1b-it\"\n",
        "DATASET_ID = \"fredmo/gemma_ft_dc_rules_dataset\"\n",
        "\n",
        "# Define local directories for outputs\n",
        "TRAINING_OUTPUT_DIR = \"./results\"\n",
        "MERGED_MODEL_DIR = \"./merged_fine_tuned_model\"\n",
        "BASE_ANSWER_FILE = \"base_model_answer.txt\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(TRAINING_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MERGED_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Helper function to clear GPU memory\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Frees up GPU memory.\"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# --- 4. Base Model Inference ---\n",
        "# Generate an answer from the base model to use as a baseline for comparison.\n",
        "print(\"\\n--- Performing base inference ---\")\n",
        "llm_base = None\n",
        "try:\n",
        "    # Configure and load the base model using vLLM for efficient inference\n",
        "    llm_base = LLM(\n",
        "        model=BASE_MODEL_ID,\n",
        "        trust_remote_code=True,\n",
        "        dtype=torch.float16,\n",
        "        gpu_memory_utilization=0.5 # Use less memory to be safe\n",
        "    )\n",
        "\n",
        "    question = \"What are Cloud Front Ends and how do they relate to customer VMs in Google Cloud?\"\n",
        "    sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=256)\n",
        "\n",
        "    # The instruction-tuned (-it) model uses a chat template, which we apply here\n",
        "    tokenizer_base = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    prompt = tokenizer_base.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Generate the answer and save it to a local file\n",
        "    outputs = llm_base.generate([prompt], sampling_params)\n",
        "    base_model_answer = outputs[0].outputs[0].text.strip()\n",
        "    with open(BASE_ANSWER_FILE, \"w\") as f:\n",
        "        f.write(base_model_answer)\n",
        "    print(f\"\\nBase model answer saved to {BASE_ANSWER_FILE}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up to free GPU memory\n",
        "    if llm_base:\n",
        "        del llm_base\n",
        "    clear_gpu_memory()\n",
        "    print(\"Base inference complete and resources cleared.\")\n",
        "\n",
        "# --- 5. Fine-Tuning ---\n",
        "# Fine-tune the base model using the specified dataset.\n",
        "print(\"\\n--- Performing fine-tuning ---\")\n",
        "trainer = None\n",
        "model = None\n",
        "try:\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(DATASET_ID, split=\"train\").shuffle(seed=42)\n",
        "\n",
        "    # Configure BitsAndBytes for 4-bit quantization to save memory\n",
        "    compute_dtype = torch.float16\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype\n",
        "    )\n",
        "\n",
        "    # Load the model with the quantization config\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    # Configure the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Configure LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        r=64,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Configure training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=TRAINING_OUTPUT_DIR,\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        optim=\"adamw_torch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=25,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        max_grad_norm=0.3,\n",
        "        warmup_ratio=0.03,\n",
        "        group_by_length=True,\n",
        "        lr_scheduler_type=\"constant\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Format the dataset using the chat template\n",
        "    formatted_dataset = dataset.map(lambda ex: {\"text\": tokenizer.apply_chat_template(ex[\"messages\"], tokenize=False)})\n",
        "\n",
        "    # Initialize and run the trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=formatted_dataset,\n",
        "        peft_config=peft_config,\n",
        "        args=training_args\n",
        "    )\n",
        "    trainer.train()\n",
        "    print(\"\\nFine-tuning complete.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up resources\n",
        "    if 'model' in locals():\n",
        "        del model\n",
        "    if 'trainer' in locals():\n",
        "        del trainer\n",
        "    clear_gpu_memory()\n",
        "    print(\"Fine-tuning resources cleared.\")\n",
        "\n",
        "# --- 6. Merge Adapter and Save Model ---\n",
        "# Find the latest adapter checkpoint from training\n",
        "checkpoint_dirs = [d for d in os.listdir(TRAINING_OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n",
        "latest_checkpoint_name = max(checkpoint_dirs, key=lambda d: int(re.search(r\"(\\d+)\", d).group(1)))\n",
        "ADAPTER_PATH = os.path.join(TRAINING_OUTPUT_DIR, latest_checkpoint_name)\n",
        "print(f\"Using adapter from: {ADAPTER_PATH}\")\n",
        "\n",
        "# Merge the fine-tuned LoRA adapter into the base model to create a full model\n",
        "print(\"\\n--- Merging adapter to create a full model ---\")\n",
        "try:\n",
        "    # Load the base model in float16\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    # Apply the adapter to the base model\n",
        "    merged_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "    # Merge the weights and unload the adapter\n",
        "    merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "    # Save the full merged model and its tokenizer to a local directory\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "    merged_model.save_pretrained(MERGED_MODEL_DIR)\n",
        "    tokenizer.save_pretrained(MERGED_MODEL_DIR)\n",
        "    print(f\"Full fine-tuned model saved to {MERGED_MODEL_DIR}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up resources\n",
        "    if 'base_model' in locals():\n",
        "        del base_model\n",
        "    if 'merged_model' in locals():\n",
        "        del merged_model\n",
        "    clear_gpu_memory()\n",
        "    print(\"Merging resources cleared.\")\n",
        "\n",
        "\n",
        "# --- 7. Inference with Fine-Tuned Model (with Fallback) ---\n",
        "print(\"\\n--- Performing inference with the fine-tuned model ---\")\n",
        "\n",
        "finetuned_model_answer = None\n",
        "\n",
        "# Primary Method: Use vLLM with the full merged model for fast inference\n",
        "try:\n",
        "    print(\"\\nAttempting to use primary method: vLLM with full merged model...\")\n",
        "    llm_finetuned = LLM(\n",
        "        model=MERGED_MODEL_DIR, # Load from the local directory\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=2048,\n",
        "        gpu_memory_utilization=0.8,\n",
        "        dtype=torch.float16,\n",
        "    )\n",
        "    tokenizer_finetuned = AutoTokenizer.from_pretrained(MERGED_MODEL_DIR)\n",
        "\n",
        "    question = \"What are Cloud Front Ends and how do they relate to customer VMs in Google Cloud?\"\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    prompt = tokenizer_finetuned.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=256)\n",
        "    outputs = llm_finetuned.generate([prompt], sampling_params)\n",
        "    finetuned_model_answer = outputs[0].outputs[0].text.strip()\n",
        "    print(\"vLLM method succeeded.\")\n",
        "    del llm_finetuned\n",
        "    clear_gpu_memory()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"vLLM method failed: {e}\")\n",
        "    finetuned_model_answer = None  # Ensure fallback runs\n",
        "\n",
        "# Fallback Method: If vLLM fails, use Transformers + PEFT with the adapter\n",
        "if finetuned_model_answer is None:\n",
        "    try:\n",
        "        print(\"\\nAttempting fallback method: Transformers + PEFT...\")\n",
        "        # Load the base model\n",
        "        base_model_peft = AutoModelForCausalLM.from_pretrained(\n",
        "            BASE_MODEL_ID,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        # Apply the LoRA adapter from the local path\n",
        "        peft_model = PeftModel.from_pretrained(base_model_peft, ADAPTER_PATH)\n",
        "        tokenizer_peft = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "        question = \"What are Cloud Front Ends and how do they relate to customer VMs in Google Cloud?\"\n",
        "        messages = [{\"role\": \"user\", \"content\": question}]\n",
        "        prompt = tokenizer_peft.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Generate the response\n",
        "        inputs = tokenizer_peft(prompt, return_tensors=\"pt\").to(peft_model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = peft_model.generate(**inputs, max_new_tokens=256, pad_token_id=tokenizer_peft.eos_token_id)\n",
        "        # Decode and remove the prompt from the output\n",
        "        response = tokenizer_peft.decode(outputs[0], skip_special_tokens=True)\n",
        "        finetuned_model_answer = response[len(prompt):].strip()\n",
        "        print(\"PEFT fallback method succeeded.\")\n",
        "        del base_model_peft, peft_model\n",
        "        clear_gpu_memory()\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"PEFT fallback also failed: {e2}\")\n",
        "        finetuned_model_answer = \"Error: Both vLLM and PEFT methods failed. Check model, adapter, or config files.\"\n",
        "\n",
        "\n",
        "# --- 8. Final Comparison ---\n",
        "# Display the expected answer alongside the answers from the base and fine-tuned models.\n",
        "\n",
        "print(\"\\n--- Reading base model answer from local file ---\")\n",
        "with open(BASE_ANSWER_FILE, \"r\") as f:\n",
        "    base_model_answer = f.read()\n",
        "\n",
        "print(\"\\n\\n\" + \"==========\" + \" FINAL RESULTS \" + \"==========\")\n",
        "print(\"\\n## Desired Answer: ##\")\n",
        "print(\"Cloud Front Ends are specific GFEs located in the same cloud region as customer VMs for minimizing latency. They allow customer VMs to communicate with Google APIs and services without needing external IP addresses.\")\n",
        "print(\"\\n## Base Model Answer: ##\")\n",
        "print(base_model_answer)\n",
        "print(\"\\n## Fine-tuned Model Answer: ##\")\n",
        "print(finetuned_model_answer)\n",
        "print(\"\\n\" + \"==========\")\n",
        "\n",
        "print(\"\\n--- Script Finished. ---\")"
      ],
      "metadata": {
        "id": "aW5r4lrU21xk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}