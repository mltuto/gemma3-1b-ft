{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV6kNAb5UxyX"
      },
      "outputs": [],
      "source": [
        "# Installation des d√©pendances\n",
        "!pip install transformers torch accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# HF login to access Gemma 3 1B which is Open Weight but a gated model (need to ack the terms before accessing)\n",
        "# Login your HuggingFace Account\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "# Execute and paste your token"
      ],
      "metadata": {
        "id": "zPl16cdfU7N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Config\n",
        "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Used device: {device}\")\n",
        "print(\"Model Loading...\")\n",
        "\n",
        "# Model config\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    load_in_8bit=True if device == \"cuda\" else False,  # Quantification pour √©conomiser la m√©moire\n",
        ")\n",
        "\n",
        "# Pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Model loading succeed!\")\n",
        "\n",
        "def generate_response(prompt, max_length=128, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generating answer\n",
        "    \"\"\"\n",
        "    # Encodage du prompt\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # G√©n√©ration de la r√©ponse\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "    # decoding answer\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response[len(prompt):].strip()\n",
        "    return response\n",
        "\n",
        "def chat_loop():\n",
        "    \"\"\"\n",
        "    interactive chat loop for testing purposes\n",
        "    \"\"\"\n",
        "    print(\"\\nüëæ Gemma3-1b-it\")\n",
        "    print(\"=\" * 25)\n",
        "    print(\"type 'quit', 'exit' or 'bye' to leave\")\n",
        "    print(\"type 'clear' to wipe history\")\n",
        "    print(\"=\" * 25)\n",
        "\n",
        "    conversation_history = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # User input\n",
        "            user_input = input(\"\\nüëΩ You: \").strip()\n",
        "\n",
        "            # Commandes sp√©ciales\n",
        "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "                print(\"\\nüëã Bye!\")\n",
        "                break\n",
        "\n",
        "            if user_input.lower() == 'clear':\n",
        "                conversation_history = []\n",
        "                print(\"üßπ history wiped!\")\n",
        "                continue\n",
        "\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            # Construction du prompt avec l'historique\n",
        "            if conversation_history:\n",
        "                context = \"\\n\".join(conversation_history[-6:])  # Garder les 6 derniers √©changes\n",
        "                prompt = f\"{context}\\nUser: {user_input}\\nAssistant:\"\n",
        "            else:\n",
        "                prompt = f\"User: {user_input}\\nAssistant:\"\n",
        "\n",
        "            print(\"\\nüëæ Gemma3-1b-it:\", end=\" \", flush=True)\n",
        "\n",
        "            # G√©n√©ration de la r√©ponse\n",
        "            response = generate_response(\n",
        "                prompt,\n",
        "                max_length=1024,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            print(response)\n",
        "\n",
        "            # Mise √† jour de l'historique\n",
        "            conversation_history.append(f\"Utilisateur: {user_input}\")\n",
        "            conversation_history.append(f\"Assistant: {response}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n‚ö†Ô∏è interruption detected. type 'quit' to leave properly.\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "# Fonction pour tester le mod√®le avec un prompt simple\n",
        "def test_model():\n",
        "    \"\"\"\n",
        "    Model Testing\n",
        "    \"\"\"\n",
        "    test_prompt = \"Hi, How can I help you? Feel free to ask me about what is Project Zero and how does it contribute to security? or How does Google ensure the security of its cloud services?\"\n",
        "    print(f\"\\nüëæ Gemma3-1b-it: '{test_prompt}'\")\n",
        "    response = generate_response(test_prompt, max_length=100)\n",
        "    print(f\"üëæ Answer: {response}\")\n",
        "\n",
        "# Lancement du test puis du chat\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()\n",
        "    chat_loop()"
      ],
      "metadata": {
        "id": "T5b8TDNZb0f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HByejLxyYcpH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}